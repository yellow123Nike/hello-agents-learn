import json
import os
from typing import Any, Dict, List, Literal, Optional, Iterator
from openai import AsyncOpenAI, OpenAI
import tiktoken

from hello_agents.core.exceptions import HelloAgentsException
from hello_agents.core.llm.llm_prompt import SENSITIVE_PATTERNS
from hello_agents.core.agent.agent_schema import AgentContext, LLMParams
from hello_agents.core.llm.message import Message
from hello_agents.core.llm.string_util import text_desensitization


class LLMClient:
    def __init__(self, params: LLMParams):
        self.params = params
        self.is_claude = params.is_claude
        self.client = AsyncOpenAI(
            api_key=params.api_key,
            base_url=params.base_url,
        )
        self.function_call_type = None

    # æ ¼å¼åŒ–æ¶ˆæ¯
    def format_messages(
        self,
        messages: List[Message],
        is_claude: bool,
    ):
        formatted = []
        for msg in messages:
            message_map: Dict[str, Any] = {}
            # ===== multimodal =====
            # 1.å¤„ç† base64 å›¾åƒ
            if msg.base64_image:
                multimodal = []
                multimodal.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{msg.base64_image}"
                    }
                })
                multimodal.append({
                    "type": "text",
                    "text": msg.content
                })

                message_map["role"] = msg.role.value
                message_map["content"] = multimodal

            # ===== tool calls =====
                # Claude æŠŠã€Œå·¥å…·è°ƒç”¨ã€å½“ä½œä¸€ç§æ¶ˆæ¯ç±»å‹ï¼ˆcontent blockï¼‰ï¼Œ
                # GPT æŠŠã€Œå·¥å…·è°ƒç”¨ã€å½“ä½œ message çš„ä¸€ä¸ªå­—æ®µï¼ˆtool_callsï¼‰
            elif msg.tool_calls:
                message_map["role"] = msg.role.value
                if is_claude:
                    claude_calls = []
                    for tc in msg.tool_calls:
                        claude_calls.append({
                            "type": "tool_use",
                            "id": tc.id,
                            "name": tc.function.name,
                            "input": json.loads(tc.function.arguments),
                        })
                    message_map["content"] = claude_calls
                else:
                    message_map["tool_calls"] = [
                        {
                            "id": tc.id,
                            "type": tc.type,
                            "function": {
                                "name": tc.function.name,
                                "arguments": tc.function.arguments,
                            }
                        }
                        for tc in msg.tool_calls
                    ]

            # ===== tool result =====
                # å·¥å…·æ‰§è¡Œç»“æœé‡æ–°å–‚ç»™å¤§æ¨¡å‹
            elif msg.tool_call_id:
                # æ‰§è¡Œç»“æœè„±æ•
                content = text_desensitization(
                    msg.content,
                    SENSITIVE_PATTERNS,
                )

                if is_claude:
                    message_map["role"] = "user"
                    message_map["content"] = [{
                        "type": "tool_result",
                        "tool_use_id": msg.tool_call_id,
                        "content": content,
                    }]
                else:
                    message_map["role"] = msg.role.value
                    message_map["content"] = content
                    message_map["tool_call_id"] = msg.tool_call_id

            # ===== normal text =====
            else:
                message_map["role"] = msg.role.value
                message_map["content"] = msg.content

            formatted.append(message_map)

        return formatted

    def count_message_tokens(self, message: Dict[str, Any]) -> int:
        """
        ç»Ÿè®¡å•æ¡ message çš„ token æ•°
        """
        try:
            encoding = tiktoken.encoding_for_model(self.params.model_name)
        except KeyError:
            # fallbackï¼ˆéå¸¸é‡è¦ï¼Œé¿å…æ¨¡å‹åä¸è¯†åˆ«ï¼‰
            encoding = tiktoken.get_encoding("cl100k_base")
        tokens = 0
        # role tokensï¼ˆOpenAI å›ºå®šå¼€é”€ï¼‰
        tokens += 4  # æ¯æ¡ message çš„ç»“æ„å¼€é”€
        content = message.get("content", "")
        if isinstance(content, list):
            for item in content:
                if item.get("type") == "text":
                    tokens += len(encoding.encode(item.get("text", "")))
                elif item.get("type") == "image_url":
                    tokens += 85
        else:
            tokens += len(encoding.encode(str(content)))

        return tokens

    # tokenæˆªæ–­:å€’åºè´ªå¿ƒ+userè¾¹ç•Œå¯¹é½
    def truncate_message(
        self,
        context: AgentContext,
        messages: List[Dict[str, Any]],
        max_input_tokens: int
    ):
        if not messages or max_input_tokens < 0:
            return messages

        truncated_messages: List[Dict[str, Any]] = []
        remaining_tokens = max_input_tokens

        system = messages[0]
        if system.get("role") == "system":
            remaining_tokens -= self.count_message_tokens(system)

        # ä»åå¾€å‰å–
        for message in reversed(messages):
            message_tokens = self.count_message_tokens(message)
            if remaining_tokens >= message_tokens:
                truncated_messages.insert(0, message)
                remaining_tokens -= message_tokens
            else:
                break

        # ä¿è¯ç¬¬ä¸€æ¡é system æ¶ˆæ¯æ˜¯ user
        while truncated_messages:
            first = truncated_messages[0]
            if first.get("role") != "user":
                truncated_messages.pop(0)
            else:
                break

        if system.get("role") == "system":
            truncated_messages.insert(0, system)

        return truncated_messages

    def _prepare_messages(
        self,
        context: AgentContext,
        messages: List[Message],
        system_msgs: Optional[Message],
    ) -> List[dict]:
        # -------- 1.1 æ ¼å¼åŒ– messages --------
        if system_msgs:
            formatted_system_msgs = self.format_messages(
                [system_msgs],
                is_claude=self.is_claude,
            )
            formatted_messages = list(formatted_system_msgs)
            formatted_messages.extend(
                self.format_messages(messages, is_claude=self.is_claude)
            )
        else:
            formatted_messages = self.format_messages(
                messages,
                is_claude=self.is_claude,
            )
        # -------- 1.2 æˆªæ–­è¾“å…¥ --------
        if self.params.max_tokens is not None:
            formatted_messages = self.truncate_message(
                context=context,
                messages=formatted_messages,
                max_input_tokens=self.params.max_tokens,
            )

        return formatted_messages

    # function_call-param
    def add_function_name_param(
        self,
        parameters: Dict[str, Any],
        tool_name: str,
    ):
        """
        """
        new_parameters = copy.deepcopy(parameters)
        new_required = ["function_name"]
        if "required" in parameters and parameters["required"] is not None:
            new_required.extend(parameters["required"])
        new_parameters["required"] = new_required
        new_properties: Dict[str, Any] = {}

        function_name_map = {
            "description": f"é»˜è®¤å€¼ä¸ºå·¥å…·å: {tool_name}",
            "type": "string",
        }
        new_properties["function_name"] = function_name_map

        if "properties" in parameters and parameters["properties"] is not None:
            new_properties.update(parameters["properties"])

        new_parameters["properties"] = new_properties

        return new_parameters

    def to_openai_tool_choice(
        self,
        tool_choice: ToolChoice,
        forced_tool_name: Optional[str] = None,
    ):
        """
        å°†å†…éƒ¨ ToolChoice æ˜ å°„ä¸º OpenAI chat.completions çš„ tool_choice å‚æ•°ã€‚
        - NONE/AUTO: ç›´æ¥è¿”å›å­—ç¬¦ä¸²
        - REQUIRED:
            - å¦‚æœæŒ‡å®š forced_tool_nameï¼šè¿”å›å¼ºåˆ¶è°ƒç”¨æŸå·¥å…·çš„ object
            - å¦åˆ™ï¼šé™çº§ä¸º "auto"ï¼ˆé¿å… OpenAI ä¸æ”¯æŒ "required" å¯¼è‡´æŠ¥é”™ï¼‰
        """
        if tool_choice == ToolChoice.NONE:
            return "none"
        if tool_choice == ToolChoice.AUTO:
            return "auto"

        # REQUIRED
        if forced_tool_name:
            return {"type": "function", "function": {"name": forced_tool_name}}

        # æ— æ³•æ˜ç¡®å¼ºåˆ¶å“ªä¸€ä¸ªå·¥å…·æ—¶ï¼Œä¸å»ºè®®ä¼  "required"
        # å› ä¸º chat.completions æœªå¿…æ¥å—ï¼›é™çº§ä¸º auto æ›´ç¨³
        return "auto"

    async def ask_llm_once(
        self,
        context: AgentContext,
        messages: List[Message],
        system_msgs: Optional[List[Message]] = None,
    ) -> str:
        try:
            formatted_messages = self._prepare_messages(
                context, messages, system_msgs
            )

            params = {"messages": formatted_messages, "stream": False}

            response = await self.call_openai(params)

            if (
                not response
                or not response.choices
                or response.choices[0].message.content is None
            ):
                raise ValueError("Empty or invalid response from LLM")

            return response.choices[0].message.content

        except Exception:
            logger.exception("%s ask_llm_once failed", context.request_id)
            raise

    async def ask_llm_stream(
        self,
        context: AgentContext,
        messages: List[Message],
        system_msgs: Optional[List[Message]] = None,
    ):
        try:
            formatted_messages = self._prepare_messages(
                context, messages, system_msgs
            )

            params = {"messages": formatted_messages, "stream": True}

            async for chunk in self.call_openai_stream(params):
                yield chunk

        except Exception:
            logger.exception("%s ask_llm_stream failed", context.request_id)
            raise

    def _normalize_enum(self, value, enum_cls, name: str):
        if isinstance(value, enum_cls):
            return value
        if isinstance(value, str):
            try:
                return enum_cls(value)
            except ValueError:
                raise ValueError(
                    f"Invalid {name}: {value}, "
                    f"must be one of {[e.value for e in enum_cls]}"
                )
        raise TypeError(
            f"{name} must be {enum_cls.__name__} or str, got {type(value)}"
        )

    async def ask_tool(
        self,
        context: AgentContext,
        messages: List[Message],
        tools: ToolCollection,
        tool_choice: ToolChoice | str,
        system_msgs: Optional[Message],
        function_call_type: FunctionCallType | str = FunctionCallType.FUNCTION_CALL
    ) -> ToolCallResponse:
        try:
            # ===== 1. ToolChoice æ ¡éªŒ=====
            self.function_call_type = function_call_type
            tool_choice = self._normalize_enum(
                tool_choice, ToolChoice, "tool_choice")
            function_call_type = self._normalize_enum(
                function_call_type,
                FunctionCallType,
                "function_call_type"
            )
            start_time = time.time()
            # ===== 2. æ„é€  OpenAI toolsï¼ˆå¯¹é½ function_call åˆ†æ”¯ï¼‰ =====
            formatted_tools: list[dict] = []
            string_builder: list[str] = []
            if function_call_type is FunctionCallType.STRUCT_PARSE:
                # ===== struct_parse åˆ†æ”¯ =====
                string_builder.append(STRUCT_PARSE_TOOL_SYSTEM_PROMPT)
                # ---------- base tool ----------
                for tool in tools.tool_map.values():
                    function_map = {
                        "name": tool.name,
                        "description": tool.description,
                        "parameters": self.add_function_name_param(
                            tool.to_params(),
                            tool.name,
                        ),
                    }
                    string_builder.append(
                        f"- `{tool.name}`\n```json {json.dumps(function_map, ensure_ascii=False)} ```\n"
                    )

                # ---------- mcp tool ----------
                for tool in tools.mcp_tool_map.values():
                    parameters = json.loads(tool.parameters)
                    function_map = {
                        "name": tool.name,
                        "description": tool.desc,
                        "parameters": self.add_function_name_param(
                            parameters,
                            tool.name,
                        ),
                    }
                    string_builder.append(
                        f"- `{tool.name}`\n```json {json.dumps(function_map, ensure_ascii=False)} ```\n"
                    )
                struct_prompt = "\n".join(string_builder)
                system_msgs.content = (
                    (system_msgs.content or "")
                    + "\n"
                    + struct_prompt
                )
            else:
                # ========= base tool =========
                for tool in tools.tool_map.values():
                    function_map = {
                        "name": tool.name,
                        "description": tool.description,
                        "parameters": tool.to_params(),  # æ³¨æ„ï¼šæ²¡æœ‰ add_function_name_param
                    }

                    tool_map = {
                        "type": "function",
                        "function": function_map,
                    }

                    formatted_tools.append(tool_map)

                # ========= mcp tool =========
                for tool in tools.mcp_tool_map.values():
                    parameters = json.loads(tool.parameters)

                    function_map = {
                        "name": tool.name,
                        "description": tool.desc,
                        "parameters": parameters,
                    }

                    tool_map = {
                        "type": "function",
                        "function": function_map,
                    }

                    formatted_tools.append(tool_map)

            # ===== 3. æ ¼å¼åŒ–æ¶ˆæ¯ =====
            formatted_messages = self._prepare_messages(
                context, messages, system_msgs
            )

            # ===== 4. è°ƒç”¨ OpenAI =====
            response = await asyncio.wait_for(self.client.chat.completions.create(
                model=self.params.model_name,
                messages=formatted_messages,
                tools=formatted_tools,
                tool_choice=self.to_openai_tool_choice(tool_choice),
                temperature=self.params.temperature,
                max_tokens=self.params.max_tokens,
            ),
                timeout=240,
            )

            # ===== 5. è§£æå“åº” =====
            if not response.choices or response.choices[0].message is None:
                raise ValueError("Invalid or empty response from LLM")

            choice = response.choices[0]
            message = choice.message

            content = message.content if message.content != "null" else None
            tool_calls: List["ToolCall"] = []
            if function_call_type is FunctionCallType.STRUCT_PARSE:
                pattern = r"```json\s*([\s\S]*?)\s*```"
                content = re.findall(pattern, content or "")
                for json_block in content:
                    try:
                        data = json.loads(json_block)
                        tool_name = data.pop("function_name", None)
                        if not tool_name:
                            continue

                        tool_calls.append(
                            ToolCall(
                                id=str(uuid.uuid4()),
                                type="function",
                                function=ToolCall.Function(
                                    name=tool_name,
                                    arguments=json.dumps(
                                        data, ensure_ascii=False),
                                ),
                            )
                        )
                    except Exception:
                        # å¯¹é½ Javaï¼šè§£æå¤±è´¥ç›´æ¥å¿½ç•¥
                        continue
            else:
                if message.tool_calls:
                    for tc in message.tool_calls:
                        tool_calls.append(
                            ToolCall(
                                id=tc.id,
                                type=tc.type,
                                function=ToolCall.Function(
                                    name=tc.function.name,
                                    arguments=tc.function.arguments,
                                ),
                            )
                        )
            finish_reason = choice.finish_reason
            # ===== usage =====
            total_tokens = response.usage.total_tokens if response.usage else None
            # ===== duration =====
            duration_ms = int((time.time() - start_time) * 1000)
            return ToolCallResponse(
                content=content,
                tool_calls=tool_calls,
                finish_reason=finish_reason,
                total_tokens=total_tokens,
                duration=duration_ms,
            )

        except Exception as e:
            print(f"%s Unexpected error in ask_tool: %s",
                  context.request_id,
                  str(e),
                  )
            raise

    @retry(
        stop=stop_after_attempt(3),                 # æœ€å¤šé‡è¯• 3 æ¬¡
        wait=wait_exponential(multiplier=1, min=2, max=10),  # æŒ‡æ•°é€€é¿
        retry=retry_if_exception_type(
            (
                RateLimitError,
                APIConnectionError,
                Timeout,
                asyncio.TimeoutError,
            )
        ),
        reraise=True,   # æœ€ç»ˆå¤±è´¥æ—¶æŠ›å‡ºåŸå¼‚å¸¸
    )
    async def call_openai(
        self,
        params
    ):
        response = await asyncio.wait_for(self.client.chat.completions.create(
            model=self.params.model_name,
            messages=params["messages"],
            temperature=self.params.temperature,
            max_tokens=self.params.max_tokens,
            stream=params["stream"],
        ), timeout=240)

        return response

    @retry(
        stop=stop_after_attempt(3),                 # æœ€å¤šé‡è¯• 3 æ¬¡
        wait=wait_exponential(multiplier=1, min=2, max=10),  # æŒ‡æ•°é€€é¿
        retry=retry_if_exception_type(
            (
                RateLimitError,
                APIConnectionError,
                Timeout,
                asyncio.TimeoutError,
            )
        ),
        reraise=True,   # æœ€ç»ˆå¤±è´¥æ—¶æŠ›å‡ºåŸå¼‚å¸¸
    )
    async def call_openai_stream(
        self,
        params
    ):
        response = await asyncio.wait_for(self.client.chat.completions.create(
            model=self.params.model_name,
            messages=params["messages"],
            temperature=self.params.temperature,
            max_tokens=self.params.max_tokens,
            stream=params["stream"],
        ), timeout=240)

        async for event in response:
            choice = event.choices[0]
            delta = choice.delta

            if delta and delta.content:
                yield delta.content


# æ”¯æŒçš„LLMæä¾›å•†
SUPPORTED_PROVIDERS = Literal[
    "openai",
    "deepseek",
    "qwen",
    "modelscope",
    "kimi",
    "zhipu",
    "ollama",
    "vllm",
    "local",
    "auto",
    "custom",
]


class HelloAgentsLLM:
    """
    ä¸ºHelloAgentså®šåˆ¶çš„LLMå®¢æˆ·ç«¯ã€‚
    å®ƒç”¨äºè°ƒç”¨ä»»ä½•å…¼å®¹OpenAIæ¥å£çš„æœåŠ¡ï¼Œå¹¶é»˜è®¤ä½¿ç”¨æµå¼å“åº”ã€‚

    è®¾è®¡ç†å¿µï¼š
    - å‚æ•°ä¼˜å…ˆï¼Œç¯å¢ƒå˜é‡å…œåº•
    - æµå¼å“åº”ä¸ºé»˜è®¤ï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ
    - æ”¯æŒå¤šç§LLMæä¾›å•†
    - ç»Ÿä¸€çš„è°ƒç”¨æ¥å£
    """

    def __init__(
        self,
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        provider: Optional[SUPPORTED_PROVIDERS] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        timeout: Optional[int] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯ã€‚ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå¦‚æœæœªæä¾›ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½ã€‚
        æ”¯æŒè‡ªåŠ¨æ£€æµ‹provideræˆ–ä½¿ç”¨ç»Ÿä¸€çš„LLM_*ç¯å¢ƒå˜é‡é…ç½®ã€‚

        Args:
            model: æ¨¡å‹åç§°ï¼Œå¦‚æœæœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡LLM_MODEL_IDè¯»å–
            api_key: APIå¯†é’¥ï¼Œå¦‚æœæœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            base_url: æœåŠ¡åœ°å€ï¼Œå¦‚æœæœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡LLM_BASE_URLè¯»å–
            provider: LLMæä¾›å•†ï¼Œå¦‚æœæœªæä¾›åˆ™è‡ªåŠ¨æ£€æµ‹
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            timeout: è¶…æ—¶æ—¶é—´ï¼Œä»ç¯å¢ƒå˜é‡LLM_TIMEOUTè¯»å–ï¼Œé»˜è®¤60ç§’
        """
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå¦‚æœæœªæä¾›ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½
        self.model = model or os.getenv("LLM_MODEL_ID")
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout or int(os.getenv("LLM_TIMEOUT", "60"))
        self.kwargs = kwargs

        # è‡ªåŠ¨æ£€æµ‹provideræˆ–ä½¿ç”¨æŒ‡å®šçš„provider
        requested_provider = (provider or "").lower() if provider else None
        self.provider = provider or self._auto_detect_provider(
            api_key, base_url)

        if requested_provider == "custom":
            self.provider = "custom"
            self.api_key = api_key or os.getenv("LLM_API_KEY")
            self.base_url = base_url or os.getenv("LLM_BASE_URL")
        else:
            # æ ¹æ®providerç¡®å®šAPIå¯†é’¥å’Œbase_url
            self.api_key, self.base_url = self._resolve_credentials(
                api_key, base_url)

        # éªŒè¯å¿…è¦å‚æ•°
        if not self.model:
            self.model = self._get_default_model()
        if not all([self.api_key, self.base_url]):
            raise HelloAgentsException("APIå¯†é’¥å’ŒæœåŠ¡åœ°å€å¿…é¡»è¢«æä¾›æˆ–åœ¨.envæ–‡ä»¶ä¸­å®šä¹‰ã€‚")

        # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
        self._client = self._create_client()

    def _auto_detect_provider(self, api_key: Optional[str], base_url: Optional[str]) -> str:
        """
        è‡ªåŠ¨æ£€æµ‹LLMæä¾›å•†

        æ£€æµ‹é€»è¾‘ï¼š
        1. ä¼˜å…ˆæ£€æŸ¥ç‰¹å®šæä¾›å•†çš„ç¯å¢ƒå˜é‡
        2. æ ¹æ®APIå¯†é’¥æ ¼å¼åˆ¤æ–­
        3. æ ¹æ®base_urlåˆ¤æ–­
        4. é»˜è®¤è¿”å›é€šç”¨é…ç½®
        """
        # 1. æ£€æŸ¥ç‰¹å®šæä¾›å•†çš„ç¯å¢ƒå˜é‡
        if os.getenv("OPENAI_API_KEY"):
            return "openai"
        if os.getenv("DEEPSEEK_API_KEY"):
            return "deepseek"
        if os.getenv("DASHSCOPE_API_KEY"):
            return "qwen"
        if os.getenv("MODELSCOPE_API_KEY"):
            return "modelscope"
        if os.getenv("KIMI_API_KEY") or os.getenv("MOONSHOT_API_KEY"):
            return "kimi"
        if os.getenv("ZHIPU_API_KEY") or os.getenv("GLM_API_KEY"):
            return "zhipu"
        if os.getenv("OLLAMA_API_KEY") or os.getenv("OLLAMA_HOST"):
            return "ollama"
        if os.getenv("VLLM_API_KEY") or os.getenv("VLLM_HOST"):
            return "vllm"

        # 2. æ ¹æ®APIå¯†é’¥æ ¼å¼åˆ¤æ–­
        actual_api_key = api_key or os.getenv("LLM_API_KEY")
        if actual_api_key:
            actual_key_lower = actual_api_key.lower()
            if actual_api_key.startswith("ms-"):
                return "modelscope"
            elif actual_key_lower == "ollama":
                return "ollama"
            elif actual_key_lower == "vllm":
                return "vllm"
            elif actual_key_lower == "local":
                return "local"
            elif actual_api_key.startswith("sk-") and len(actual_api_key) > 50:
                # å¯èƒ½æ˜¯OpenAIã€DeepSeekæˆ–Kimiï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ¤æ–­
                pass
            elif actual_api_key.endswith(".") or "." in actual_api_key[-20:]:
                # æ™ºè°±AIçš„APIå¯†é’¥æ ¼å¼é€šå¸¸åŒ…å«ç‚¹å·
                return "zhipu"

        # 3. æ ¹æ®base_urlåˆ¤æ–­
        actual_base_url = base_url or os.getenv("LLM_BASE_URL")
        if actual_base_url:
            base_url_lower = actual_base_url.lower()
            if "api.openai.com" in base_url_lower:
                return "openai"
            elif "api.deepseek.com" in base_url_lower:
                return "deepseek"
            elif "dashscope.aliyuncs.com" in base_url_lower:
                return "qwen"
            elif "api-inference.modelscope.cn" in base_url_lower:
                return "modelscope"
            elif "api.moonshot.cn" in base_url_lower:
                return "kimi"
            elif "open.bigmodel.cn" in base_url_lower:
                return "zhipu"
            elif "localhost" in base_url_lower or "127.0.0.1" in base_url_lower:
                # æœ¬åœ°éƒ¨ç½²æ£€æµ‹ - ä¼˜å…ˆæ£€æŸ¥ç‰¹å®šæœåŠ¡
                if ":11434" in base_url_lower or "ollama" in base_url_lower:
                    return "ollama"
                elif ":8000" in base_url_lower and "vllm" in base_url_lower:
                    return "vllm"
                elif ":8080" in base_url_lower or ":7860" in base_url_lower:
                    return "local"
                else:
                    # æ ¹æ®APIå¯†é’¥è¿›ä¸€æ­¥åˆ¤æ–­
                    if actual_api_key and actual_api_key.lower() == "ollama":
                        return "ollama"
                    elif actual_api_key and actual_api_key.lower() == "vllm":
                        return "vllm"
                    else:
                        return "local"
            elif any(port in base_url_lower for port in [":8080", ":7860", ":5000"]):
                # å¸¸è§çš„æœ¬åœ°éƒ¨ç½²ç«¯å£
                return "local"

        # 4. é»˜è®¤è¿”å›autoï¼Œä½¿ç”¨é€šç”¨é…ç½®
        return "auto"

    def _resolve_credentials(self, api_key: Optional[str], base_url: Optional[str]) -> tuple[str, str]:
        """æ ¹æ®providerè§£æAPIå¯†é’¥å’Œbase_url"""
        if self.provider == "openai":
            resolved_api_key = api_key or os.getenv(
                "OPENAI_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv(
                "LLM_BASE_URL") or "https://api.openai.com/v1"
            return resolved_api_key, resolved_base_url

        elif self.provider == "deepseek":
            resolved_api_key = api_key or os.getenv(
                "DEEPSEEK_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv(
                "LLM_BASE_URL") or "https://api.deepseek.com"
            return resolved_api_key, resolved_base_url

        elif self.provider == "qwen":
            resolved_api_key = api_key or os.getenv(
                "DASHSCOPE_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv(
                "LLM_BASE_URL") or "https://dashscope.aliyuncs.com/compatible-mode/v1"
            return resolved_api_key, resolved_base_url

        elif self.provider == "modelscope":
            resolved_api_key = api_key or os.getenv(
                "MODELSCOPE_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv(
                "LLM_BASE_URL") or "https://api-inference.modelscope.cn/v1/"
            return resolved_api_key, resolved_base_url

        elif self.provider == "kimi":
            resolved_api_key = api_key or os.getenv("KIMI_API_KEY") or os.getenv(
                "MOONSHOT_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv(
                "LLM_BASE_URL") or "https://api.moonshot.cn/v1"
            return resolved_api_key, resolved_base_url

        elif self.provider == "zhipu":
            resolved_api_key = api_key or os.getenv("ZHIPU_API_KEY") or os.getenv(
                "GLM_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv(
                "LLM_BASE_URL") or "https://open.bigmodel.cn/api/paas/v4"
            return resolved_api_key, resolved_base_url

        elif self.provider == "ollama":
            resolved_api_key = api_key or os.getenv(
                "OLLAMA_API_KEY") or os.getenv("LLM_API_KEY") or "ollama"
            resolved_base_url = base_url or os.getenv("OLLAMA_HOST") or os.getenv(
                "LLM_BASE_URL") or "http://localhost:11434/v1"
            return resolved_api_key, resolved_base_url

        elif self.provider == "vllm":
            resolved_api_key = api_key or os.getenv(
                "VLLM_API_KEY") or os.getenv("LLM_API_KEY") or "vllm"
            resolved_base_url = base_url or os.getenv("VLLM_HOST") or os.getenv(
                "LLM_BASE_URL") or "http://localhost:8000/v1"
            return resolved_api_key, resolved_base_url

        elif self.provider == "local":
            resolved_api_key = api_key or os.getenv("LLM_API_KEY") or "local"
            resolved_base_url = base_url or os.getenv(
                "LLM_BASE_URL") or "http://localhost:8000/v1"
            return resolved_api_key, resolved_base_url

        elif self.provider == "custom":
            resolved_api_key = api_key or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL")
            return resolved_api_key, resolved_base_url

        else:
            # autoæˆ–å…¶ä»–æƒ…å†µï¼šä½¿ç”¨é€šç”¨é…ç½®ï¼Œæ”¯æŒä»»ä½•OpenAIå…¼å®¹çš„æœåŠ¡
            resolved_api_key = api_key or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL")
            return resolved_api_key, resolved_base_url

    def _create_client(self) -> OpenAI:
        """åˆ›å»ºOpenAIå®¢æˆ·ç«¯"""
        return OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            timeout=self.timeout
        )

    def _get_default_model(self) -> str:
        """è·å–é»˜è®¤æ¨¡å‹"""
        if self.provider == "openai":
            return "gpt-3.5-turbo"
        elif self.provider == "deepseek":
            return "deepseek-chat"
        elif self.provider == "qwen":
            return "qwen-plus"
        elif self.provider == "modelscope":
            return "Qwen/Qwen2.5-72B-Instruct"
        elif self.provider == "kimi":
            return "moonshot-v1-8k"
        elif self.provider == "zhipu":
            return "glm-4"
        elif self.provider == "ollama":
            return "llama3.2"  # Ollamaå¸¸ç”¨æ¨¡å‹
        elif self.provider == "vllm":
            return "meta-llama/Llama-2-7b-chat-hf"  # vLLMå¸¸ç”¨æ¨¡å‹
        elif self.provider == "local":
            return "local-model"  # æœ¬åœ°æ¨¡å‹å ä½ç¬¦
        elif self.provider == "custom":
            return self.model or "gpt-3.5-turbo"
        else:
            # autoæˆ–å…¶ä»–æƒ…å†µï¼šæ ¹æ®base_urlæ™ºèƒ½æ¨æ–­é»˜è®¤æ¨¡å‹
            base_url = os.getenv("LLM_BASE_URL", "")
            base_url_lower = base_url.lower()
            if "modelscope" in base_url_lower:
                return "Qwen/Qwen2.5-72B-Instruct"
            elif "deepseek" in base_url_lower:
                return "deepseek-chat"
            elif "dashscope" in base_url_lower:
                return "qwen-plus"
            elif "moonshot" in base_url_lower:
                return "moonshot-v1-8k"
            elif "bigmodel" in base_url_lower:
                return "glm-4"
            elif "ollama" in base_url_lower or ":11434" in base_url_lower:
                return "llama3.2"
            elif ":8000" in base_url_lower or "vllm" in base_url_lower:
                return "meta-llama/Llama-2-7b-chat-hf"
            elif "localhost" in base_url_lower or "127.0.0.1" in base_url_lower:
                return "local-model"
            else:
                return "gpt-3.5-turbo"

    def think(self, messages: list[dict[str, str]], temperature: Optional[float] = None) -> Iterator[str]:
        """
        è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ€è€ƒï¼Œå¹¶è¿”å›æµå¼å“åº”ã€‚
        è¿™æ˜¯ä¸»è¦çš„è°ƒç”¨æ–¹æ³•ï¼Œé»˜è®¤ä½¿ç”¨æµå¼å“åº”ä»¥è·å¾—æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„å€¼

        Yields:
            str: æµå¼å“åº”çš„æ–‡æœ¬ç‰‡æ®µ
        """
        print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...")
        try:
            response = self._client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature if temperature is not None else self.temperature,
                max_tokens=self.max_tokens,
                stream=True,
            )

            # å¤„ç†æµå¼å“åº”
            print("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:")
            for chunk in response:
                content = chunk.choices[0].delta.content or ""
                if content:
                    print(content, end="", flush=True)
                    yield content
            print()  # åœ¨æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ

        except Exception as e:
            print(f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise HelloAgentsException(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")

    def invoke(self, messages: list[dict[str, str]], **kwargs) -> str:
        """
        éæµå¼è°ƒç”¨LLMï¼Œè¿”å›å®Œæ•´å“åº”ã€‚
        é€‚ç”¨äºä¸éœ€è¦æµå¼è¾“å‡ºçš„åœºæ™¯ã€‚
        """
        try:
            response = self._client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=kwargs.get('temperature', self.temperature),
                max_tokens=kwargs.get('max_tokens', self.max_tokens),
                **{k: v for k, v in kwargs.items() if k not in ['temperature', 'max_tokens']}
            )
            return response.choices[0].message.content
        except Exception as e:
            raise HelloAgentsException(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")

    def stream_invoke(self, messages: list[dict[str, str]], **kwargs) -> Iterator[str]:
        """
        æµå¼è°ƒç”¨LLMçš„åˆ«åæ–¹æ³•ï¼Œä¸thinkæ–¹æ³•åŠŸèƒ½ç›¸åŒã€‚
        ä¿æŒå‘åå…¼å®¹æ€§ã€‚
        """
        temperature = kwargs.get('temperature')
        yield from self.think(messages, temperature)
